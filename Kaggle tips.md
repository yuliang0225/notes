# Kaggle tips
#study/kaggle
- [Kaggleで学ぶ系列データのための深層学習モデリング](https://speakerdeck.com/yu4u/kaggletexue-huxi-lie-tetanotamenoshen-ceng-xue-xi-moterinku?slide=39)
- [LLM_Prompt_Recovery](https://speakerdeck.com/payanotty/llm-prompt-recovery)
- [Kaggle Grandmaster振り返り](https://zenn.dev/yume_neko/articles/314ab559048aa8)
- https://github.com/SyunkiTakase/ViT_Classification_Sample/tree/main
- [timmを利用したVision Transformerの学習](https://qiita.com/syunki_tacase/items/73ac0708638c67b46c07)
- [yuweihao/MambaOut: MambaOut: Do We Really Need Mamba for Vision?](https://github.com/yuweihao/MambaOut?tab=readme-ov-file)
- [SyunkiTakase/ViT_Classification_Sample: ViTベースの手法による画像分類のサンプルコード](https://github.com/SyunkiTakase/ViT_Classification_Sample/tree/main)
- Kaggle kando
- [第2回関東kaggler会 LT コンペ振り返りのすすめ](https://speakerdeck.com/yumeneko/di-2hui-guan-dong-kagglerhui-lt-konpezhen-rifan-rinosusume)
- [Kaggleで賞金200万円ゲットした話 〜 Machine Unlearning と Google AI Assistants 〜](https://speakerdeck.com/toshi_k_datasci/kaggledeshang-jin-200mo-yuan-getutositahua-machine-unlearning-to-google-ai-assistants?slide=12)
- [睡眠コンペ 1st place solution](https://speakerdeck.com/unonao/shui-mian-konpe-1st-place-solution)
- https://speakerdeck.com/k951286/kaggleyi-li-tiaitemushao-jie-ru-men-bian
- [【第1回】関東Kaggler会「データ分析コンペとの向き合い方」](https://speakerdeck.com/takaito/di-1hui-guan-dong-kagglerhui-lt4-detafen-xi-konpetonoxiang-kihe-ifang)
- [もしKagglerがスクラムを学んだら_関東Kaggler会\#2](https://speakerdeck.com/sue1242/mosikagglerkasukuramuwoxue-ntara-guan-dong-kagglerhui-number-2?slide=3)
- ML contests: [ML Contests](https://mlcontests.com/)
- [yunsuxiaozi/AI-and-competition: 这里用来存储做人工智能项目的代码和参加数据挖掘比赛的代码](https://github.com/yunsuxiaozi/AI-and-competition)
- [【データ解析】Adversarial Validationについて](https://tomatosauce.jp/adversarial_validation/)
  - [JongdaeHan/Adversarial_validation](https://github.com/JongdaeHan/Adversarial_validation)
  - https://www.acceluniverse.com/blog/developers/2020/01/kaggleadversarial-validation.html
- text reorder
  - [gioelecrispo/text-reorderer: This notebook provides a method to reorder the sentences in a text, using BERT For Next Sentence Prediction.](https://github.com/gioelecrispo/text-reorderer?tab=readme-ov-file)
  - https://github.com/danielenapo/Sentence-reordering-with-Transformers/blob/main/Napolitano_Daniele_DeepLearning_10_06_23.ipynb
- Pseudo labeling
  - [Exploring Pseudolabelling Schemes - PyData](https://www.kaggle.com/code/stanleyjzheng/exploring-pseudolabelling-schemes-pydata)
  - https://github.com/YudeWang/Learning-Pseudo-Label?tab=readme-ov-file
- DS protfolio
  - [Erlemar.github.io](https://erlemar.github.io/)
## Kaggle master talk machine learning
- [競技としてのKaggle、役に立つKaggle](https://speakerdeck.com/yu4u/jing-ji-tositenokaggle-yi-nili-tukaggle?slide=8)
![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.39.19.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-11-23%2020.46.32.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-11-23%2020.46.22.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-11-23%2020.46.48.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-11-23%2020.49.05.png)
![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.47.17.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.50.31.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.50.44.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-11-23%2020.50.34.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.53.24.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.54.31.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.55.22.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.56.15.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.56.36.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2019.57.11.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2020.02.09.png)![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-04-25%2020.02.54.png)
## **Kaggle Grandmasterになったのでこれまでの参加コンペを振り返る**
- [Kaggle Grandmasterになったのでこれまでの参加コンペを振り返る - yu4uの日記](https://yu4u.hatenadiary.org/entry/2023/01/15/185119)
- 1 month after
  - Start with most vote notbook and discussion
  - Simple notebook
  - Idea —> CV —> Submit
- **Humpback Whale Identification**
  - https://www.kaggle.com/competitions/humpback-whale-identification/discussion/82366
  - https://www.slideshare.net/slideshow/humpback-whale-identification-challenge/135193722
- **Recursion Cellular Image Classification**
  - https://www.kaggle.com/competitions/recursion-cellular-image-classification/discussion/110337
  - 割当問題でラベルを最適化してpseudo labelで学習するのを繰り返すという力技でした。
- 


---
## **[Kaggleテーブルデータコンペできっと役立つTipsまとめ](https://naotaka1128.hatenadiary.jp/entry/kaggle-compe-tips)**
- https://naotaka1128.hatenadiary.jp/entry/kaggle-compe-tips
### EDA
- Tableau would be better
- Data check: Scatter
  - Train test from same distribution or not
- Histogram
- `df.dtypes, df.info, x.value_counts, x.isnull`
### Data cleaning
- Rm useless columns
  - one value
  - useless
  - Corr == 1
  - https://www.kaggle.com/code/yuansun/lb-0-84-for-starters
  - `pd.factorize`: [機械学習でのカテゴリデータの扱い方](https://qiita.com/QUANON/items/08a65012366abd150172)
  - `df.select_dtypes(include=['object']).columns`
- Scaling
  - MinMaxScaler ⇔ StandardScaler
  - kNN
- Outliers
  - Clipping 99%
```python
upperbound, lowerbound = np.percentile(x, [1, 99])
y = np.clip(x, upperbound, lowerbound)
```
- rank transformation
  - kNN and NN
  - `scipy.stats.rankdata`
- log transformation
  - NN
  - `np.log1p(x)` or `np.sqrt(x+2/3)`
### Numerical faetures
- More is better
  - count, sum, max, min, rolling
  - is_null, is_zero, is_not_zero, over_0.5
  - Nan count, 0 count, - count
- between 2 columns
  - A - B
  - category mean
  - PolynomialFeatures: [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)
- 小数点以下だけ取り出した要素とか作ろう
- Tree based model
  - 積み上げの値を普通の値に戻す (線形モデルでは不要)
  - 和、差、積など取るのは重要だから頑張ろう
    - 差や比率を直接表現できないため
    - GBM only APPROXIMATE interactions and non-linear transformations.
    - Strong interactions benefit from being explicitly defined.
- Numerical —> Categorical 
![](Kaggle%20tips/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202024-08-02%2019.03.55.png)<!-- {"width":691.87682926829268} -->
### Categorical / Ordinal data
- Encodings
  - NN —> onehotencoding
  - Tree model —> No used
- FrequencyEncoding
  - NN, Tree —> OK
  - take log1p(Freq)
- TargetEncoding
  - Care about data leak
  - https://www.slideshare.net/slideshow/feature-engineering-83511751/83511751
  - Smoothing: [nilci - TargetEncodingのスムーシング](https://tori29.jp/blog/export_28)
  - ![](Kaggle%20tips/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202024-08-02%2019.08.28.png)
- One leave out category encode
  - [Target Encodingで精度向上させた例\(Leave One Out\)](https://qiita.com/FukuharaYohei/items/88cc562ab7800cbc01cb)
  - [Feature Engineering](https://www.slideshare.net/slideshow/feature-engineering-83511751/83511751)
  - [Count Encoder — Category Encoders 2.6.4 documentation](https://contrib.scikit-learn.org/category_encoders/count.html)
![](Kaggle%20tips/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202024-12-06%2012.37.24.png)<!-- {"width":442} -->
- Likelihood encoding and Catboost encoding
  - [Kaggle_meetup__4.pdf](Kaggle%20tips/Kaggle_meetup__4.pdf)<!-- {"embed":"true"} -->
  - Likelihood encoding —> mean of target with oof
  - Concat categories = Concat(c1, c2, c3) —> encoding
    - [XGBOOST with combination of factors](https://www.kaggle.com/code/rsakata/xgboost-with-combination-of-factors)
    - [1706](https://arxiv.org/pdf/1706.09516)
![](Kaggle%20tips/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202024-12-06%2012.49.59.png)![](Kaggle%20tips/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202024-12-06%2012.51.03.png)

- Target encode with noise
  - https://www.kaggle.com/code/ogrellier/python-target-encoding-for-categorical-features/notebook
```python
def add_noise(series, noise_level):
    return series * (1 + noise_level * np.random.randn(len(series)))

def target_encode(trn_series=None, 
                  tst_series=None, 
                  target=None, 
                  min_samples_leaf=1, 
                  smoothing=1,
                  noise_level=0):
    """
    Smoothing is computed like in the following paper by Daniele Micci-Barreca
    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf
    trn_series : training categorical feature as a pd.Series
    tst_series : test categorical feature as a pd.Series
    target : target data as a pd.Series
    min_samples_leaf (int) : minimum samples to take category average into account
    smoothing (int) : smoothing effect to balance categorical average vs prior  
    """ 
    assert len(trn_series) == len(target)
    assert trn_series.name == tst_series.name
    temp = pd.concat([trn_series, target], axis=1)
    # Compute target mean 
    averages = temp.groupby(by=trn_series.name)[target.name].agg(["mean", "count"])
    # Compute smoothing
    smoothing = 1 / (1 + np.exp(-(averages["count"] - min_samples_leaf) / smoothing))
    # Apply average function to all target data
    prior = target.mean()
    # The bigger the count the less full_avg is taken into account
    averages[target.name] = prior * (1 - smoothing) + averages["mean"] * smoothing
    averages.drop(["mean", "count"], axis=1, inplace=True)
    # Apply averages to trn and tst series
    ft_trn_series = pd.merge(
        trn_series.to_frame(trn_series.name),
        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),
        on=trn_series.name,
        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)
    # pd.merge does not keep the index so restore it
    ft_trn_series.index = trn_series.index 
    ft_tst_series = pd.merge(
        tst_series.to_frame(tst_series.name),
        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),
        on=tst_series.name,
        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)
    # pd.merge does not keep the index so restore it
    ft_tst_series.index = tst_series.index
    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)
```
### Others
- PCA, NMF —> Tree model
  - https://pyroomacoustics.readthedocs.io/en/pypi-release/pyroomacoustics.bss.fastmnmf.html
- Topic model —> LDA, Sbert
- Isolation score
  - kNN neighbor count 
    - [upura/knnFeat: Python Implementation of Feature Extraction with K-Nearest Neighbor](https://github.com/upura/knnFeat)
    - [momijiame/gokinjo: gokinjo: A feature extraction library based on k-nearest neighbor algorithm in Python](https://github.com/momijiame/gokinjo)
  - isolation forest score
  - k-means center distance
- Add red score —> train again
- diff prediction
- LibFM
  - [LibFM_in_Keras/keras_blog.ipynb at master · jfpuget/LibFM_in_Keras](https://github.com/jfpuget/LibFM_in_Keras/blob/master/keras_blog.ipynb)

### Imbalance
- [scikit-learn-contrib/imbalanced-learn: A Python Package to Tackle the Curse of Imbalanced Datasets in Machine Learning](https://github.com/scikit-learn-contrib/imbalanced-learn)
- LGBM class weight
### Ensemble and Stacking
- http://mlwave.com/kaggle-ensembling-guide/
- GBMと相性が良いモデル
  * RandomForest(2)
  * Neural Network(理由: Tree Baseアルゴリズムは決定境界が特徴軸に平行な矩形になるが、NNなどは曲線(曲面)となるため(2))
  * Glmnet (RidgeやLASSO)(8)
* 
### Reduce memory
- np.memmap & h5py
- np.packbits
- TPU
- https://www.kaggle.com/code/gemartin/load-data-reduce-memory-usage
```python
def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    
    return df


def import_data(file):
    """create a dataframe and optimize its memory usage"""
    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)
    df = reduce_mem_usage(df)
    return df
```
![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-08-04%2021.51.05.png)
- paper with code
---
## Kaggle books
- [ghmagazine/kagglebook](https://github.com/ghmagazine/kagglebook)
- [Packt.The.Kaggle.Book.2022.4.pdf](Kaggle%20tips/Packt.The.Kaggle.Book.2022.4.pdf)<!-- {"embed":"true","width":235} -->
- querying the Meta Kaggle dataset
  - https://www.kaggle.com/datasets/kaggle/meta-kaggle
- [Heads or Tails | Grandmaster](https://www.kaggle.com/headsortails)
  - EDA is important than a model
- DIY metric. We can quote a few examples of this approach as used by Kagglers:
  - [How to compute the gradient and hessian of logarithmic loss? \(question is based on a numpy example script from xgboost's github repository\)](https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based)
  - Focal loss
    - [\[論文紹介\] Focal Loss for Dense Object Detection](https://qiita.com/agatan/items/53fe8d21f2147b0ac982)
  - Carlo Lepelaars with Spearman’s Rho:
    - https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho
  - Carlo Lepelaars with Quadratic Weighted Kappa:
    - https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa
  - Rohan Rao with Laplace Log Likelihood:
    - https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood
  - https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279
  - https://petamind.com/advanced-keras-custom-loss-functions/
  - https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/
  - Focal loss
    - a loss that aims to heavily weight the minority class in the loss computations as described in Lin, T-Y. et al. Focal loss for dense object detection.
    - [Focal loss implementation for LightGBM](https://maxhalford.github.io/blog/lightgbm-focal-loss/)
```python
from scipy.misc import derivative
import xgboost as xgb

def focal_loss(alpha, gamma):
	def loss_func(y_pred, y_true):
		a, g = alpha, gamma
	def get_loss(y_pred, y_true):
		p = 1 / (1 + np.exp(-y_pred))
		loss = (-(a * y_true + (1 - a)*(1 - y_true)) *
				((1 - (y_true * p + (1 - y_true) *
				(1 - p)))**g) * (y_true * np.log(p) +
				(1 - y_true) * np.log(1 - p)))
		return loss

	partial_focal = lambda y_pred: get_loss(y_pred, y_true)
	grad = derivative(partial_focal, y_pred, n=1, dx=1e-6)
	hess = derivative(partial_focal, y_pred, n=2, dx=1e-6)
	return grad, hess
return loss_func

xgb = xgb.XGBClassifier(objective=focal_loss(alpha=0.25, gamma=1))
```
- Post-processing 
  - by transforming the regression results into integers
  - SciPy’soptimize.minimize —> Nelder-Mead algorithm
  - https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107
  - [Post-Processing Technique \(c.f. 1st Place Jigsaw\)](https://www.kaggle.com/code/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw)
  - [Postprocessing based on leakage](https://www.kaggle.com/code/tomooinubushi/postprocessing-based-on-leakage)
  - https://www.kaggle.com/code/saitodevel01/indoor-post-processing-by-cost-minimization
- [Vopani | Grandmaster](https://www.kaggle.com/rohanrao)
  - For any Kaggle competition, my typical workflow would look like this:
    - Understand the problem statement and read all the information related to rules, format, timelines, datasets, metrics, and deliverables.
    - Dive deep into the data. Slice and dice it in every way possible and explore/visualize it to be able to answer any question about it.
    - Build a simple pipeline with a baseline model and make a submission to confirm the process works.
    - Engineer features, tune hyperparameters, and experiment with multiple models to get a sense of what’s generally working and what’s not.
    - Constantly go back to analyzing the data, reading discussions on the forum, and tweaking the features and models to the fullest. Maybe team up at some point.
    - Ensemble multiple models and decide which submissions to make as final
  - In my day-to-day work in data science, most of this happens too. But there are two crucial elements that are additionally required:
    - Curating and preparing datasets for the problem statement.
    - Deploying the final model or solution into production.
- [Andrew Lukyanenko | Grandmaster](https://www.kaggle.com/artgor)
  - Those who are inexperienced in general make a number of different mistakes 
    * 最严重的问题之一：缺乏批判性思维，不知道如何进行自己的研究。
    * 不知道何时以及使用哪些工具/方法。
    * 盲目地拿来公共笔记本并使用，而不了解它们的工作原理。
    * 固守某个想法，花费过多时间去追求，即使它不起作用。
    * 当实验失败时感到绝望并失去动力。
  * 对于那些有数据科学经验但没有Kaggle经验的人，我认为他们忽视的最严重的问题是低估了Kaggle的难度。
    * 他们没有预料到Kaggle的竞争非常激烈，成功需要尝试许多不同的方法，他们也没有意识到有很多只有在比赛中才有效的技巧，还有一些人是专业参与比赛的。
    * 此外，人们常常高估领域专家的作用。我承认，确实有一些比赛中，拥有领域专家的团队赢得了金奖和奖金，但在大多数情况下，经验丰富的Kaggle选手会获胜。
    * 另外，我多次看到以下情况：有些人宣称赢得Kaggle很容易，并且他（或他所在的小组）将在不久的将来获得金奖或多个金奖。但在大多数情况下，他们悄无声息地失败了。
  * What mistakes have you made in competitions in the past?
    * 没有足够地深入数据。有时候由于这个原因，我无法生成更好的特征或应用更好的后处理。
      * 反向工程和“黄金特征”是另一个完全不同的话题。
    * 花费太多时间在一个单一的想法上，因为我希望它能奏效。这被称为沉没成本谬误。
    * 实验不够。
      * 努力是有回报的——如果你没有在比赛上花费足够的时间和资源，就无法在排行榜上取得高排名。
    * 参加了“错误”的比赛。有些比赛有数据泄漏、反向工程等问题。
      * 也有一些比赛在公共和私人测试数据之间有不合理的划分，并且确保了混乱。
      * 有些比赛对我来说并不够有趣，我本不应该开始参与。
    * 和错误的人组队。有时候我的队友没有像我预期的那样活跃，导致了团队分数的下降。
    * 至于你准备投入什么，通常是指你愿意投入的时间和精力，以及你拥有的硬件设备。
      * 当我说到结果时，我指的是比赛结束后会发生什么。
      * 你可能会在这场比赛中投入大量时间并获胜，但也可能会失败。
      * 你准备好面对这个现实吗？赢得某个特定比赛对你来说有多重要？也许你需要准备投入更多的努力；
      * 另一方面，也许你有长期目标，一场失败的比赛不会对你造成太大影响。
- [Larxel | Grandmaster](https://www.kaggle.com/andrewmvd)
  - 在一次关于 **X 光图像中结核病检测** 的竞赛中，我们只有大约 **1,000 张图像**，这个数量对于捕捉结核病的所有表现形式来说非常少。为了解决这个问题，我提出了两个思路：
    1. **在外部肺炎检测数据上进行预训练**（大约 **20,000 张图像**）。
       - 因为肺炎的症状有时会被误认为是结核病，通过在肺炎数据上进行预训练，可以帮助模型更好地识别结核病特征。
    2. **在包含多种肺部异常的多标签分类数据上进行预训练**（大约 **60 万张图像**）。
       - 之后，使用 **grad-CAM（梯度加权类别激活映射）** 结合简单的 **SSD（单阶段检测器）** 生成用于分类标签的边界框注释。
       - 这一方法有助于模型更准确地聚焦于肺部的异常区域。
    - 最终，采用这两种方法的简单融合，比获得第二名的团队的结果高出了 **22%**。这次比赛是在一个医疗会议上举办的，约有 **100 支队伍** 参加。
  - 你有没有特别推荐用于数据分析/机器学习的工具或库？
    - 说来奇怪，我既推荐也不推荐一些库。
    - **LightGBM** 是一个优秀的用于表格数据的机器学习库，性能与计算时间的比率非常出色。
    - **CatBoost** 有时可以超过 LightGBM 的表现，但代价是需要更多的计算时间，而这些时间本可以用来测试新的想法。
    - **Optuna** 非常适合超参数调优，**Streamlit** 用于前端展示，**Gradio** 用于构建 MVP（最小可行产品），**FastAPI** 用于构建微服务，**Plotly 和 Plotly Express** 用于绘制图表，**PyTorch 及其衍生库** 适用于深度学习。
    - 虽然这些库非常有用，但我也建议在你的职业生涯中某个阶段自己去实现这些算法。
      - 我第一次听到这个建议是来自 **Andrew Ng**，后来也听到许多同等级别的大师提到过。
      - 自己实现这些算法可以让你获得非常深入的理解，能够更好地理解模型的工作原理，以及它对参数调整、数据和噪声的反应。
  - 在你的经验中，新手 Kaggle 选手常常忽视什么？如果能重来，你希望在刚开始时知道什么？
    - 多年来，我最希望自己能更早意识到的是以下几点：
      1. **在比赛结束后吸收所有的知识**
      2. **复现已结束比赛中的获胜方案**
    - 在比赛接近尾声的压力下，排行榜的变动会比以往任何时候都要频繁。
      - 这种情况下，你更不可能冒险或有时间仔细研究所有细节。
      - 而在比赛结束后，你不再有这种紧迫感，可以慢慢地分析和理解。
      - 你还可以复现那些公开了方案的获胜者的思路。
    - 如果你能坚持这样做，这对提升你的数据科学技能会有极大的帮助。
      - 关键是：**比赛结束时不要停下，而是当你真正掌握了所有知识时再停下**。
      - 我也曾在 Andrew Ng 的一次演讲中听到过类似的建议，他提到**复现论文是成为 AI 专家的最佳途径之一**。
    - 另外，在比赛结束时，你可能已经筋疲力尽，只想赶紧结束。
      - 这没问题，但请记住，**比赛结束后的讨论区可能是地球上知识最丰富的地方之一**，因为很多获胜方案的代码和思路都会公开。
      - 花时间去阅读和学习那些获胜者是如何做的，千万不要急着去做其他事情，否则你可能会错过一次极好的学习机会。
  - 参加比赛时，最重要的事情是什么？
    - 假设你的主要目标是成长与发展，我的建议是：**选择一个你感兴趣的话题和你从未做过的任务的比赛**。
    - 批判性思维和专业能力需要**深度和多样性**。专注于比赛并全力以赴可以保证你在某一领域的深度，而通过尝试新任务或用不同的方法解决问题，可以获得多样性。
- [Vopani | Grandmaster](https://www.kaggle.com/rohanrao)
  - 你是如何参与 Kaggle 比赛的？这种方法和你日常工作中有什么不同？
    - 对于任何 Kaggle 比赛，我通常会按以下流程进行：
      * **理解问题描述**：仔细阅读与比赛相关的所有信息，包括规则、格式、时间线、数据集、评估指标和交付要求。
      * **深入数据分析**：对数据进行多角度的切分和探索，使用可视化手段，力求回答关于数据的任何问题。
      * **建立基线模型**：构建一个简单的管道，并使用基线模型进行提交，确认整个流程可以正常运作。
      * **特征工程与参数调优**：设计新的特征、调优超参数，尝试多种模型，以了解哪些方法有效，哪些无效。
      * **反复分析与调整**：不断回到数据分析，阅读论坛讨论，最大限度地调整特征和模型。有时也会选择与他人组队。
      * **模型集成与提交决策**：通过集成多种模型，决定最终的提交方案。
    * 在我日常的数据科学工作中，以上流程大部分也会涉及。但除此之外，还有两个关键的额外环节：
      1. **数据集的整理与准备**：在实际项目中，往往需要花大量时间来清洗、整理并准备适合问题需求的数据集。
      2. **模型的部署与上线**：比赛只关注模型的效果，而在实际工作中，更重要的是如何将最终模型或解决方案部署到生产环境中去。
    - 事实上，我过去大部分项目的时间都花在了这两个环节上。
- [Firat Gonen | Grandmaster](https://www.kaggle.com/frtgnn)
- [Giba | Grandmaster](https://www.kaggle.com/titericz)
  - 自从 2011 年开始在 Kaggle 上参赛以来，我最喜欢的比赛类型是那些带有结构化表格数据的比赛。
    - 在 Kaggle 上，我用得最多的技术是类别特征的目标编码（target encoding）（有无数种错误的做法）和堆叠集成（stacking ensembles）。
  - Kaggle 是一个用于机器学习的绝佳练习场。与现实生活中的项目不同，Kaggle 上的问题通常已经被很好地定义和格式化，数据集已经创建，目标变量也已经构建，评估指标也已选定。
    - 所以，我总是从探索性数据分析 (EDA) 开始 Kaggle 比赛。理解问题并熟悉数据集是获得优势的关键之一。
    - 在此之后，我会花一些时间来定义一个合适的验证策略。
    - 这对于正确验证模型，并且与 Kaggle 评估私有测试集的方式保持一致，非常重要。
    - 尽管分层 K 折 (stratified K-fold) 在大多数二分类问题中都有效，但我们必须评估是否应该使用基于分组的 K 折 (grouped K-fold) 或基于时间的拆分 (time-based split)，以便正确验证，避免过拟合，并尽可能地模拟私有测试集。
    - 接下来，我会花一些时间进行特征工程和超参数优化实验。
    - 此外，我通常会在比赛结束时至少使用一种梯度提升树 (Gradient Boosted Tree) 模型和一种基于深度学习的方法。
    - 将这些不同方法进行融合（blend）对于提高预测的多样性和提升比赛指标非常重要。
  - 在 2016 年之前，我一直从事电子工程工作，凭借自 2011 年以来在 Kaggle 上学到的一切，我成功地转型到了数据科学领域。
    - Kaggle 帮助我理解了机器学习的概念，并应用了从理论中学到的一切。
    - 此外，Kaggle 是一个极好的实验场所，你可以下载数据集，进行各种尝试，尽可能地挖掘数据中的信息。
    - 这种结合了竞赛环境的学习方式，非常适合学习编码和机器学习，同时也非常容易上瘾，让你不断想学更多。
    - 赢得几次比赛可以让你的名字登上排行榜，这对于职业生涯来说是无价的。
    - 全世界的猎头都会关注 Kaggle，以寻找适合职位的人才，而通过比赛积累的知识和经验可以极大地推动职业发展。
  - 自从我加入 Kaggle 以来，我花了好几年时间学习各种技术、算法和技巧，努力从数据中提取更多信息，并尽可能提高指标。
    - 高准确率是大多数比赛的主要目标，但仅靠运气几乎不可能做到；知识和经验在目标是获胜或者至少拿到金牌区域时起到了非常重要的作用。
    - 我在 Kaggle 比赛中获得的奖牌数量就是我的作品集；截至目前（2021 年 11 月），我已经获得了 58 枚金牌和 47 枚银牌，这很好地总结了我在 Kaggle 上获得的机器学习经验。
    - 考虑到每场比赛至少持续 1 个月，这意味着我已经积累了超过 105 个月的连续竞赛经验。
  - 新手往往忽视了合适的验证策略。这不仅仅发生在 Kaggle 上，我见过世界各地的数据科学家在构建模型时都忽略了实验理论中最重要的事情之一。
    - 在设置合适的验证策略时没有通用的规则，但数据科学家必须考虑到模型未来的使用方式，并尽可能让验证过程与未来的使用场景接近。
  - 犯过很多错误，几乎不可能全部列出来。我可能已经犯过所有可能的错误组合。
    - 错误的好处在于你可以从中学习。一旦你犯了一个错误并且检测到了它，那么你很可能不会再犯同样的错误。
    - 在 Kaggle 上，大家犯的主要错误是过于信任排行榜分数，而不是自己本地验证的分数。
    - 在 Kaggle 上过拟合排行榜分数是一个常见现象，这也是 Kaggle 与现实世界之间的主要区别。
    - 在真实的项目中，我们必须建立一个强有力的验证策略，因为在现实世界中，模型会在真实数据上进行测试，而且你只有一次机会去命中目标，而不是每天可以多次提交。
  - 几年前，我可能会推荐 R，但考虑到 Python 在机器学习领域的增长速度之快以及它在生产环境中使用的泛用性和易用性，我建议任何开始学习机器学习的人都学习 Python。
    - 对于表格数据的库，我推荐使用 pandas 进行数据操作，如果你想要更快的速度，那么可以使用 cuDF（RAPIDS.ai 提供的 GPU 版本的 pandas）。
    - 对于 EDA（探索性数据分析），我推荐使用 DataFrame 结合 Seaborn 或 Matplotlib 进行可视化。
    - 在机器学习方面，我推荐使用 Scikit-learn、SciPy、cuML（GPU 版）、XGBoost、LightGBM、CatBoost 和 PyTorch。
    - 需要记住的是，使用原始特征快速构建一个简单的 XGBoost 模型通常可以作为一个很好的基准，用来与后续的模型进行比较。
  - 参加 Kaggle 比赛并提交一个公开的 Notebook 很容易，但想要在比赛中拿到金牌区域的成绩是非常有挑战性的。
    - 所以，对我来说，最重要的事情是，无论最终排名如何，我们都应该利用 Kaggle 去享受比赛的乐趣，并尽可能多地从讨论区、公开的 Notebook，甚至是比赛结束后的获胜者帖子中学习到有价值的内容。
    - 还需要记住的是，赢得比赛的不仅仅是复制其他人正在做的事情，而是要跳出框框，提出一些新的想法、策略、架构和方法。
  - 

### Time feature processing
```python
cycle = 7
df['weekday_sin'] = np.sin(2 * np.pi * df['col1'].dt.dayofweek / cycle)
df['weekday_cos'] = np.cos(2 * np.pi * df['col1'].dt.dayofweek / cycle)
```
### Numeric feature transformations
- Scaling; normalization; logarithmic or exponential transformations; 
- separating the integer and decimal parts; summing, subtracting, multiplying, or dividing two numeric features. 
- Scaling obtained by standardization (the z-score method used in statistics) or by normalization (also called min-max scaling) of numeric features can make sense if you are using algorithms sensitive to the scale of features, such as any neural network.
### Binning of numeric features
- This is used to transform continuous variables into discrete ones by distributing their values into a number of bins. 
- Binning helps remove noise and errors in data and it allows easy modeling of non-linear relationships between the binned features and the target variable when paired with one-hot encoding
- [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html)
### Categorical feature encoding
- One-hot encoding; a categorical data processing that merges two or three categorical features together;
- or the more sophisticated target encoding (more on this in the following sections).
### Splitting and aggregating categorical features based on the levels
### Polynomial features
- [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)
### Missing values treatment
- replace the missing values with the mean, median, or mode
- https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python
![](Kaggle%20tips/image%208.png)
### Outlier capping or removal
- cap to a maximum or minimum value
- https://scikit-learn.org/stable/modules/outlier_detection.html
- interquartile range (IQR)
### Meta-features based on rows and columns
- A good place to start is looking at features based on each row
  - Compute the mean, median, sum, standard deviation, minimum, or maximum of the numeric values (or of a subset of them)
  - Count the missing values
  - Compute the frequencies of common values found in the rows (for instance, considering the binary features and counting the positive values)
  - Assign each row to a cluster derived from a cluster analysis such as k-means
- As meta-features, you can use any kind of column statistic (such as mode, mean, median, sum, standard deviation, min, max, and also skewness and kurtosis for numerical features)
  - Frequency encoding
  - Frequencies and column statistics computed with respect to a relevant group
  - https://www.kaggle.com/code/lucamassaron/meta-features-and-target-encoding
```python
# Frequency count of a feature
feature_counts = train.groupby('ROLE_TITLE').size()
print(train['ROLE_TITLE'].apply(lambda x: feature_counts[x]))
```
- **Meta-features and target encoding**
  - [Target Encoder — Category Encoders 2.8.0 documentation](https://contrib.scikit-learn.org/category_encoders/targetencoder.html)
```python
from sklearn.base import BaseEstimator, TransformerMixin

class TargetEncode(BaseEstimator, TransformerMixin):
    
    def __init__(self, categories='auto', k=1, f=1, 
                 noise_level=0, random_state=None):
        if type(categories)==str and categories!='auto':
            self.categories = [categories]
        else:
            self.categories = categories
        self.k = k
        self.f = f
        self.noise_level = noise_level
        self.encodings = dict()
        self.prior = None
        self.random_state = random_state
        
    def add_noise(self, series, noise_level):
        return series * (1 + noise_level *   
                         np.random.randn(len(series)))
        
    def fit(self, X, y=None):
        if type(self.categories)=='auto':
            self.categories = np.where(X.dtypes == type(object()))[0]
        
        temp = X.loc[:, self.categories].copy()
        temp['target'] = y
        self.prior = np.mean(y)
        for variable in self.categories:
            avg = (temp.groupby(by=variable)['target']
                       .agg(['mean', 'count']))
            # Compute smoothing 
            smoothing = (1 / (1 + np.exp(-(avg['count'] - self.k) /                 
                         self.f)))
            # The bigger the count the less full_avg is accounted
            self.encodings[variable] = dict(self.prior * (1 -  
                             smoothing) + avg['mean'] * smoothing)
            
        return self
    
    def transform(self, X):
        Xt = X.copy()
        for variable in self.categories:
            Xt[variable].replace(self.encodings[variable], 
                                 inplace=True)
            unknown_value = {value:self.prior for value in 
                             X[variable].unique() 
                             if value not in 
                             self.encodings[variable].keys()}
            if len(unknown_value) > 0:
                Xt[variable].replace(unknown_value, inplace=True)
            Xt[variable] = Xt[variable].astype(float)
            if self.noise_level > 0:
                if self.random_state is not None:
                    np.random.seed(self.random_state)
                Xt[variable] = self.add_noise(Xt[variable], 
                                              self.noise_level)
        return Xt
    
    def fit_transform(self, X, y=None):
        self.fit(X, y)
        return self.transform(X)
```
### Using feature importance to evaluate your work
- Based on our experiences, we suggest you consider placing feature selection at the end of your data preparation pipeline. 
- For regression models, using lasso selection can provide a hint about all the important yet correlated features
- Faeture importance from GDBT
- https://www.kaggle.com/code/cdeotte/lstm-feature-importance
- Boruta: [scikit-learn-contrib/boruta_py: Python implementations of the Boruta all-relevant feature selection method.](https://github.com/scikit-learn-contrib/boruta_py)
- BorutaShap: https://github.com/Ekeany/Boruta-Shap
  - [\[Tutorial\] Feature selection with Boruta-SHAP](https://www.kaggle.com/code/lucamassaron/tutorial-feature-selection-with-boruta-shap)
### Pseudo-labeling
- pseudo-labeling can boost your scores by providing further examples taken from the test set.
- Generally, this is the procedure:
- [Pseudo Labeling - QDA - \[0.969\]](https://www.kaggle.com/code/cdeotte/pseudo-labeling-qda-0-969)
  1. Train your model
  2. Predict on the test set
  3. Establish a confidence measure
  4. Select the test set elements to add
  5. Build a new model with the combined data
  6. Predict using this model and submit
- You should have a very good model that produces good predictions for them to be usable in training. Otherwise, you will just add more noise.
- Since it is impossible to have entirely perfect predictions in the test set, you need to distinguish the good ones from the ones you shouldn’t use. 
- In the second stage, when you concatenate the training examples with the test ones, do not put in more than 50% test examples.
### Denoising with autoencoders
- https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629
- https://github.com/ryancheunggit/Denoise-Transformer-AutoEncoder
- https://www.kaggle.com/code/hungkhoi/train-denoise-transformer-autoencoder
- https://www.kaggle.com/code/jeongyoonlee/dae-with-2-lines-of-code-with-kaggler

- [CPMP | Grandmaster](https://www.kaggle.com/cpmpml)
  - 首先，我会尽可能深入地了解数据。
    - 我尝试在数据中找到规律，尤其是具有预测性的规律。
    - 我经常做的事情是使用两个特征或衍生特征分别作为 x 和 y 轴，第三个特征用来对样本进行颜色编码进行可视化，其中一个特征可以是目标特征。
    - 我使用大量的可视化工具，因为我认为人类的视觉是最强大的数据分析工具。
  - 其次，我花时间在如何评估模型或管道的性能上。
    - 的确，尽可能准确地评估模型的性能是极其重要的。
    - 这一点并不令人意外；通常使用的评估方法是 k 折交叉验证的某种变体。
    - 但折叠的定义可以根据比赛的类型进行定制（例如，预测比赛使用基于时间的折叠，当样本因某种原因关联在一起时（例如，具有相同用户 ID 的行为），则使用分组 k 折验证）。
  - 然后，我创建一个从数据到提交的端到端基线模型，并进行测试。
    - 如果这是一个代码竞赛，那么验证你的管道是否正确是关键。
  - 接下来，我尝试使用更复杂的模型（如果使用深度学习模型）或者更多的特征（如果使用 XGBoost 或 RAPIDS、sklearn 等库的其他模型）。
    - 我提交这些模型，看看本地评估分数与公共测试分数之间是否有相关性。如果相关性良好，我就会减少提交次数。
  - 几周之后，我会花时间进行超参数调优。
    - 但我通常只做一次，或者最多再在比赛接近结束时做一次最后的调优。
    - 实际上，超参数调优是导致过拟合的最佳途径之一，而我非常害怕过拟合。
  - 我最自豪的比赛之一是 TalkingData AdTracking Fraud Detection Challenge。
    - 在这场比赛中，我们面对的是非常庞大的点击历史数据，需要预测哪些点击会导致应用下载。数据集的特征很少，但行数非常多（大约有五亿行）。
    - 当时我只有一台 64 GB 的机器，因此我必须实现一种非常高效的方法来创建新特征并进行评估。
  - 在这次比赛中，我有一些洞见。
    - 首先，导致应用下载的点击通常是用户在应用下载页面上的最后一次点击。
    - 因此，“同一用户在同一应用上的下一次点击时间”成为了最重要的特征。
    - 基于这个洞见，我进一步推测：存在不少来自同一用户和同一应用的具有相同时间戳的点击。
    - 我假设如果有下载发生，那么这个时间戳中最后一个点击更可能是导致下载的那个。
  - 第三个洞见是使用矩阵分解的方法来近似特征值之间的共现关系。
    - 我当时在 Keras 中实现了一个 libFM 模型，并将隐向量作为特征添加进去，这确实提高了效果。
    - 唯一使用这种方法的其他队伍是当时排名第一的队伍。
    - 凭借这些策略，我在一群 Kaggle 大师 (GM) 团队中获得了第六名的单人排名。当时我还不是 Kaggle 大师。
    - https://github.com/jfpuget/LibFM_in_Keras
  - 
- [ONODERA | Grandmaster](https://www.kaggle.com/onodera)
  - 我尝试去想象模型是如何工作的，并深入研究假阴性和假阳性的情况。这与我日常工作的方式相同。
  - **Human Protein Atlas - 单细胞分类。**
    - 这个比赛本质上是一种实例分割比赛，但没有提供掩码（masks）。因此，它变成了一种弱监督的多标签分类问题。
    - 我创建了一个用于去除标签噪声的两阶段管道。
  - 是的。我现在在 NVIDIA 的 KGMON（Kaggle Grandmasters of NVIDIA）团队工作。
    - Kaggle 发起了许多不同类型的机器学习比赛，这些比赛在数据类型（如表格数据、图像、自然语言和信号处理）以及行业和领域（如工业、金融、天文学、病理学、体育和零售等）方面各不相同。
    - 我敢打赌，除了 Kagglers 之外，几乎没有人能够接触并拥有所有这些类型数据的经验。
  - 目标分析（Target analysis）。
    - 此外，种子平均（Seed averaging）也常常被忽视：尽管总是简单却非常强大。
  - 目标分析（Target analysis）。
    - 顶级团队总是比其他人更好地分析目标。
    - 因此，如果我在比赛中没有获得更好的名次，我会去阅读顶级方案的解析，因为它们总能让我了解到在比赛期间我所忽略的数据知识。
- [Ruchi Bhatia | Grandmaster](https://www.kaggle.com/ruchi798)
  - 当一个新的比赛公布时，我的首要任务是深入理解问题陈述。
    - 有时候，问题陈述可能超出我们的舒适区或领域，因此在进入数据探索性分析（EDA）之前，确保完全理解问题至关重要。
    - 在执行EDA的过程中，我的目标是了解数据的分布，并专注于熟悉手头的数据。
    - 在此过程中，我们很可能会发现一些模式，我们应该努力去理解这些模式，并为异常值和特殊情况形成假设。
  - 接下来，我会花时间理解比赛的评价指标。
    - 之后，我会创建一个无泄漏的交叉验证策略。
    - 完成这一点后，我会选择一个基线模型并进行第一次提交。
    - 如果本地验证结果与比赛排行榜的相关性不理想，我会反复进行迭代，直到理解可能的差异并做出调整。
  - 随后，我会逐步改进我的建模方法。
    - 此外，调整参数和尝试新的实验也有助于理解什么方法最适合当前数据（在整个过程中，我都会确保防止过拟合）。最后，在比赛的最后几周，我会进行模型集成并检查解决方案的稳健性。
  - 至于我在 Kaggle 之外的项目，我的大部分时间都花在数据收集、清理以及从数据中提取有价值的信息上。
  - 根据我的经验，我注意到，许多 Kaggle 参赛者在比赛排名不如预期时会感到沮丧。
    - 经过数周甚至数月的努力，我可以理解他们为什么会早早放弃。
    - 然而，赢得 Kaggle 比赛绝非易事。
    - 参赛者中有许多来自不同教育背景和工作经验的人，勇于尝试才是最重要的。
    - 我们应该专注于自身的成长，看看自己在这段旅程中已经走了多远。
  - 综合性的探索性数据分析（EDA）结合相关的可视化，有助于我们发现数据趋势和背景，从而改进我们的方法。
    - 由于我坚信可视化的力量，所以我最喜欢的数据科学库是 **Seaborn** 和 **TensorBoard**。
    - Seaborn 用于 EDA（探索性数据分析），而 TensorBoard 则用于机器学习工作流程中的可视化。
    - 我偶尔也会使用 **Tableau**。
  - 当人们参加竞赛时，我认为他们应该做好深入理解问题陈述和进行研究的准备。
    - Kaggle 上的竞赛特别具有挑战性，并且在许多情况下有助于解决现实生活中的问题。
    - 人们应保持积极的心态，不要气馁。
    - Kaggle 竞赛提供了一个完美的机会来学习和成长！
- [Rob Mulla | Grandmaster](https://www.kaggle.com/robikscube)
  - 我认为缺乏经验的 Kagglers 有时过于担心模型的集成和超参数调优。
    - 虽然这些确实在比赛的后期很重要，但在你构建好一个基础模型之前，它们并不重要。
    - 我还认为，完全理解比赛的评估指标非常重要。
    - 许多 Kagglers 容易忽视理解如何根据评估指标来优化自己的解决方案的重要性。
  - 在 EDA（探索性数据分析）方面，我建议熟练掌握 NumPy、Pandas 以及 Matplotlib 或其他绘图库来处理数据。
    - 在建模方面，熟悉如何使用 Scikit-learn 设置正确的交叉验证方案是非常重要的。
    - 像 XGBoost 和 LightGBM 这样的标准模型非常适合用来建立基准。
    - 至于深度学习库，主要是 TensorFlow/Keras 或 PyTorch。熟悉这两个主流深度学习库中的至少一个非常重要。
- [Xavier Conort | Grandmaster](https://www.kaggle.com/xavierconort)
  - 我非常喜欢那些需要从多个表中进行特征工程来获得好结果的比赛。
    - 我喜欢挖掘出好的特征，尤其是那些我之前不熟悉的业务问题。这让我对自己解决新问题的能力充满信心。
    - 除了优秀的特征工程外，堆叠（stacking）也帮助我取得了好成绩。
    - 我用它来融合多个模型，或者将文本或高类别变量转换成数值特征。
    - 我最喜欢的算法是 GBM（梯度提升机），但我也测试了许多其他算法来为我的模型融合增加多样性。
  - 我的主要目标是从每场比赛中尽可能多地学习。
    - 在参加比赛之前，我会评估自己可以学到哪些新技能。
    - 我并不害怕走出舒适区。得益于排行榜的反馈，我知道自己可以通过错误迅速学习。
    - 然而，在日常工作中，通常很难评估我们正在处理的解决方案的实际质量。
    - 所以，我们往往会选择安全的做法，重复过去的成功经验。如果没有 Kaggle，我不认为自己能学到这么多。
  - 我最喜欢的比赛是 GE Flight Quest，这是一场由 GE 组织的比赛，参赛者需要预测美国国内航班的到达时间。
    - 我特别喜欢这场比赛的私有排行榜设计，它考验了我们预处理数据和建模的能力。
    - 由于我们只有几个月（大约 3 到 4 个月）的历史数据，我知道存在强烈的过拟合风险。
    - 为了减轻这种风险，我决定只构建那些与航班延误有明显因果关系的特征，例如衡量天气状况和交通情况的特征。
    - 同时，我非常谨慎地将机场名称排除在我的主要特征列表之外。
    - 原因在于，某些机场在这几个月的历史数据中没有遇到恶劣天气。
    - 因此，我非常担心我最喜欢的 ML 算法——GBM（梯度提升机）会利用机场名称作为良好天气的代理特征，导致在私有排行榜上这些机场的预测效果很差。
    - 为了捕捉到某些机场管理水平较好的事实，并略微提升我的排行榜分数，我最终还是使用了机场名称这个特征，但仅作为一种残差效应。
    - 这是我第二层模型中的一个特征，这些模型将第一层模型的预测结果作为偏置进行调整。
    - 这种方法可以看作是两步提升（two-step boosting），其中你在第一步中屏蔽了一些信息。我从保险行业的精算师那里学到这种方法，他们用它来捕捉地理空间的残差效应。
  - Kaggle 确实帮助了我的数据科学职业生涯。
    - 在转行成为数据科学家之前，我是一名保险行业的精算师，完全不了解机器学习，也不认识任何数据科学家。多亏了 Kaggle 多样化的比赛，我的学习曲线得到了极大提升。凭借我在比赛中的好成绩，我能够展示自己的履历，并说服雇主：一个 39 岁的精算师也可以成功地自学新技能。
    - 此外，多亏了 Kaggle 的社区，我结识了许多来自世界各地、充满激情的数据科学家。一开始，我非常享受与他们竞争或者合作的乐趣。
    - 最终，我有幸与其中一些人一起共事。DataRobot 的创始人 Jeremy Achin 和 Tom De Godoy 就是我的比赛队友，后来他们邀请我加入 DataRobot。如果没有 Kaggle 的帮助，我想我现在可能还在保险行业做精算师。
  - 我会建议缺乏经验的 Kaggle 参赛者在比赛期间不要急于查看其他人发布的解决方案，而是尝试自己找到好的解决方案。
    - 我很庆幸自己在一开始的时候就坚持这么做。
  - 一个重大错误是继续参加那些设计不佳且存在数据泄漏的比赛。
    - 参加这样的比赛只是浪费时间，因为你无法从中学到太多东西。
  - 我最喜欢的算法是 Gradient Boosting Machine (GBM, 梯度提升机)。
    - 我一开始使用的是 R 的 gbm，然后转向 Scikit-learn 的 GBM，接着是 XGBoost，最后是 LightGBM。
    - 大多数时候，GBM 都是我获胜方案的主要成分。
    - 为了深入了解 GBM 学到了什么，我强烈推荐使用 SHAP (SHapley Additive exPlanations) 包。
  - 为了学习而参赛。
    - 为了与其他热爱数据科学的人建立联系而参赛。
    - 不要只是为了赢而参赛。
- [Chris Deotte | Grandmaster](https://www.kaggle.com/cdeotte)
  - 我喜欢有趣数据的比赛以及需要构建有创意的新模型的比赛。
    - 我的专长是分析已训练好的模型，确定它们的优缺点。
    - 之后，我喜欢通过改进模型或开发后处理方法来提升交叉验证（CV）和公开排行榜（LB）的分数。
  - 我参加每场比赛的开端都是进行 EDA（探索性数据分析），建立本地验证集，构建一些简单的模型，然后提交到 Kaggle 以获取排行榜分数。
    - 这种流程能帮助我形成直觉，知道要构建一个准确且有竞争力的模型需要做些什么。
  - Kaggle 的 “Shopee – Price Match Guarantee” 是一场非常有挑战性的比赛，既需要图像模型又需要自然语言模型。
    - 一个关键的洞察是：如何从这两种模型中提取嵌入（embeddings），然后利用图像和文本信息共同来找到商品匹配关系。
  - Kaggle 帮助我提高了技能，并增强了我简历的市场价值，从而使我成为 NVIDIA 的高级数据科学家。
    - 许多雇主都会浏览 Kaggle 上的作品，寻找具备特定技能的员工来解决他们的项目需求。
    - 通过这种方式，我获得了很多工作机会。
  - 在我看来，缺乏经验的 Kagglers 通常会忽视本地验证的重要性。
    - 看到自己的名字出现在排行榜上确实令人兴奋。
    - 但是，人们往往容易关注提升排行榜分数，而不是提高交叉验证（CV）的分数。
  - 很多时候，我都会犯的一个错误是：过度信赖排行榜的分数，而不是交叉验证的分数，最终选择了错误的提交结果。
  - 当然有。进行特征工程和快速实验对于优化表格数据模型非常重要。
    - 为了加速实验和验证的周期，使用基于 GPU 的 NVIDIA RAPIDS 中的 cuDF 和 cuML 是必不可少的。
  - 最重要的是享受比赛并且学习。不要过于在意最终的排名。
    - 如果你专注于学习和享受比赛的过程，那么随着时间的推移，你的最终排名会越来越好。
- [Laura Fink | Grandmaster](https://www.kaggle.com/allunia)
  - 我最喜欢的比赛是那些能为人类带来积极影响的比赛，尤其是所有与医疗相关的挑战。
    - 尽管如此，每一场比赛对我来说都像是一场冒险，有着自己的谜题等待解开。
    - 我非常享受学习新技能和探索新类型的数据集或问题。
    - 因此，我并不专注于特定的技术，而是更关注学习新的东西。
    - 我想我之所以被人熟知，可能是因为我在探索性数据分析（EDA）方面的优势。
  - 参加比赛时，我首先会阅读问题描述和数据说明。
    - 在浏览论坛和公共 Notebooks 以收集想法后，通常我会开始开发自己的解决方案。
    - 最初，我会花一些时间进行 EDA（探索性数据分析），以寻找隐藏的群体并获得一些直觉。
    - 这对建立合理的验证策略非常有帮助，而我认为这也是所有后续步骤的基础。
    - 接下来，我会对机器学习流程的不同部分进行迭代，例如特征工程或预处理，改进模型架构，思考数据收集的问题，寻找数据泄漏，进行更多的 EDA，或者构建集成模型。
    - 我尝试用一种贪心的方式来不断改进我的解决方案。
    - Kaggle 比赛非常动态，需要尝试各种不同的想法和解决方案，才能在最后阶段存活下来。
    - 相比之下，我的日常工作更注重从数据中获得洞见，并找到简单而有效的方法来改善业务流程。
      - 这里的任务通常比使用的模型更复杂。
      - 要解决的问题必须定义得非常清楚，这意味着需要与不同背景的专家讨论，以明确要达成的目标、涉及的流程以及如何收集或融合数据。
      - 相比 Kaggle 比赛，我的日常工作需要更多的沟通，而不是机器学习技能。
  - G2Net 引力波检测比赛是我最喜欢的比赛之一。
    - 比赛的目标是从噪声中检测出模拟的引力波信号，这些噪声来源于探测器组件和地面力量。
    - 在比赛过程中，一个重要的见解是，不能盲目依赖标准的数据分析方法，而是要尝试自己的想法。
    - 在我阅读的论文中，数据主要是通过傅里叶变换或 Constant-Q 变换来处理的，之前还需要进行数据漂白和带通滤波。
    - 很快我就发现，数据漂白并没有帮助，因为它使用了功率谱密度（Power Spectral Density）的样条插值方法，而功率谱密度本身就非常嘈杂。将多项式拟合到嘈杂数据的小子集中，会因为过拟合而引入另一种错误来源。
    - 在放弃了数据漂白后，我尝试了 Constant-Q 变换的不同超参数，这种方法在论坛和公开的 Notebooks 中长期占据主导地位。由于引力波有两个不同的来源，可以用不同范围的 Q 值来覆盖，我尝试了一种在这些超参数上有所差异的模型集成方法。这确实帮助我提高了分数，但后来我遇到了瓶颈。
    - Constant-Q 变换通过对时间序列应用一系列滤波器，将其转换到频域。
      - 我开始思考，是否有一种方法可以更好、更灵活地完成这些滤波任务。就在这个时候，社区中有人提出了使用一维卷积神经网络（1D CNN）的想法，我非常喜欢。
      - 我们都知道，二维卷积神经网络（2D CNN）的滤波器可以在图像数据中检测边缘、线条和纹理。同样的事情也可以用拉普拉斯（Laplace）或索贝尔（Sobel）这样的“传统”滤波器来完成。
      - 基于这一点，我问自己：我们能不能用 1D CNN 来自动学习最重要的滤波器，而不是应用那些已经固定好的变换呢？
    - 虽然我没能让自己的 1D CNN 方案奏效，但后来发现，许多顶级团队成功地做到了这一点。尽管我没能在 G2Net 比赛中获得奖牌，但它依然是我最喜欢的比赛之一。在比赛中积累的知识以及对所谓“标准方法”的反思，都是非常宝贵的收获。
  - 我大学毕业后的第一份工作是 Java 软件开发工程师，虽然在硕士论文期间我已经接触过机器学习。
    - 我很想做更多数据分析方面的工作，但在那个时候，几乎没有“数据科学”相关的职位，或者至少它们不叫这个名字。
    - 当我第一次听说 Kaggle 时，就被它深深吸引了。
    - 从那时起，我常常晚上上 Kaggle 玩玩，纯粹是为了乐趣。
  - 我当时并没有打算换工作，但后来公司出现了一个需要机器学习技能的研究项目。由于我在 Kaggle 上积累的知识，我成功地向公司展示了自己是这个项目的合适人选。这成了我进入数据科学领域的起点。
  - Kaggle 一直是一个很好的平台，可以让我尝试新想法、学习新方法和工具，并积累实践经验。
    - 通过这种方式获得的技能，对于我在工作中的数据科学项目非常有帮助。
    - Kaggle 就像是知识的加速器，它提供了一个没有风险的沙盒环境，可以自由尝试各种想法。
    - 比赛失败至少意味着有一个教训可以吸取，而项目失败则可能对你和其他人产生巨大的负面影响。
  - 我认为，许多初次参加比赛的新人往往会被公开排行榜（public leaderboard）所迷惑，直接开始构建模型，而没有制定一个好的验证策略（validation strategy）。
    - 他们一边看着排行榜上的成绩来衡量成功与否，一边很可能正在过拟合（overfitting）到公开测试数据（public test data）。
    - 比赛结束后，这些模型往往无法泛化到未见过的私人测试数据（private test data），结果名次往往会掉落数百名。
    - 我至今还记得在 Mercedes-Benz Greener Manufacturing 比赛中有多么沮丧，因为我一直无法在公开排行榜上爬升。
    - 但是最终结果出来时，看到那么多人名次上下浮动，真是大吃一惊。
    - 从那时起，我一直牢记：**一个合适的验证方案对于处理欠拟合（underfitting）和过拟合（overfitting）的挑战非常重要。**
  - 我犯过的最大错误就是在比赛初期花了太多时间和精力在解决方案的细节上。
    - 事实上，**在建立一个合适的验证策略后，快速地迭代和尝试各种不同的想法要好得多**。
    - 这样更容易、更快地找到有前景的改进方向，而且被卡在某个点上的风险也要小得多。
  - 在 Kaggle 社区活跃时，你可以学习和练习许多常用的工具和库，我只能推荐你把它们都学一遍。
    - 保持灵活性，了解每个工具的优缺点非常重要。
    - **这样一来，你的解决方案就不会依赖于某个特定的工具，而是依赖于你的想法和创造力。**
  - 数据科学的本质不是构建模型，而是理解数据以及它是如何被收集的。
    - 我参加的许多比赛中，测试数据往往存在信息泄露（leakage）或隐藏的群体（hidden groups），而这些通常可以通过**探索性数据分析（EDA, Exploratory Data Analysis）**找到。
- [Abhishek Thakur | Grandmaster](https://www.kaggle.com/abhishek)
  - 我做的第一件事是看数据，尝试稍微理解一下。
    - 如果我参赛较晚，我会借助公开的 EDA（探索性数据分析，Exploratory Data Analysis） kernels 来帮助理解数据。
    - 我在 Kaggle 上（或者其他地方）处理问题时，首先会建立一个基准（benchmark）。
      - 建立基准非常重要，因为它为你提供了一个可以用来比较未来模型的基线（baseline）。
    - 如果我参赛较晚，在建立基准时，我尽量不借助公开的 Notebooks。
      - 如果依赖那些，我们的思维会被局限在一个方向上。至少我是这么认为的。
    - 在完成基准之后，我会尽可能地优化它，而不是马上做复杂的事情，比如 stacking（堆叠）或 blending（混合）。
      - 然后，我会重新审视数据和模型，一步一步地尝试提升基准的效果。
  - 大多数新手很容易放弃。
    - 参加 Kaggle 比赛很容易，但很容易被排名靠前的人吓到。如果新手想在 Kaggle 上取得成功，他们必须有毅力。
    - 在我看来，毅力是关键。
  - 许多新手也不敢独立开始，总是依赖公开的 kernels（代码模板）。
    - 这使得他们的思维方式会被公开 kernels 的作者所影响。
    - 我的建议是，先独立参加比赛，自己去看数据，构建特征，建立模型。
    - 然后再去看 kernels 和讨论区，看看别人是如何做的，有什么不同之处。
    - 接着，把学到的东西融入到自己的解决方案中去。
- [u++ | Master](https://www.kaggle.com/sishihara)
  - 第一步是一样的：通过探索性数据分析来思考如何解决问题。
    - Kaggle 默认使用高级机器学习方法，但在商业中情况并非如此。
    - 实际上，我会尝试找到不使用机器学习的方法。
    - 即使使用机器学习，我也更喜欢使用传统方法，比如 TF-IDF 和线性回归，而不是像 BERT 这样先进的方法。
  - 在 PetFinder.my 领养预测比赛中，提供了一个多模态数据集。
    - 许多参赛者尝试探索并使用所有类型的数据，主要的方法是从图像和文本中提取特征，拼接后用 LightGBM 进行训练。我也采用了相同的方法。
    - 令人惊讶的是，我的一位队友 takuoko（[个人主页](https://www.kaggle.com/takuok)）开发了一种非常棒的神经网络，能够端到端地处理所有数据集。
    - 经过精心设计的神经网络在多模态比赛中有可能超越 LightGBM。这是我在 2019 年学到的一课。
  - 探索性数据分析 (EDA) 的重要性。
    - 在机器学习领域，有一个叫做“无免费午餐定理”（No Free Lunch Theorem）的概念。
    - 我们不仅要学习算法，还要学会如何解决问题。
    - 无免费午餐定理的意思是：没有任何一种通用模型可以在所有问题上都表现出色。
    - 在机器学习比赛中，找到适合数据集和任务特点的模型对于提高分数是至关重要的。
  - 在公开排行榜 (Public Leaderboard) 上过度拟合。
    - 在 LANL 地震预测比赛中，我在公开排行榜上的成绩相当不错，最终排名第五。
    - 然而，我的最终排名是第 211 名，这意味着我对一个有限的数据集过于自信。
    - 过拟合是机器学习中一个非常常见的概念，通过 Kaggle 的经历，我痛苦地认识到了这一点的重要性。
  - 仔细观察训练集和验证集是如何划分的非常重要。我会尝试建立一个能复现这种划分方式的验证集。
  - 我非常喜欢 Pandas，它是处理表格数据集不可或缺的库。我用它来进行探索性数据分析，进行提取、聚合和可视化。
  - 你可以看看一些社区教程。Kaggle 也提供了一些关于 Pandas 和特征工程的学习教程课程。
### Ensembling with Blending and Stacking Solutions
- [Kaggle20ensembling20guide.685545114](https://usermanual.wiki/Document/Kaggle20ensembling20guide.685545114.pdf)
- Averaging models into an ensemble
- Majority voting
  - If the competition requires you to predict a class, you can use majority voting; that is, for each prediction, you take the class most frequently predicted by your models.
- Averaging of model predictions
  - geometric mean
  - logarithmic mean
  - harmonic mean
  - mean of powers:
```python
geometric = proba.prod(axis=1)**(1/3)
ras = roc_auc_score(y_true=y_test, y_score=geometric)
print(f"Geometric averaging ROC-AUC is: {ras:0.5f}")

harmonic = 1 / np.mean(1. / (proba + 0.00001), axis=1)
ras = roc_auc_score(y_true=y_test, y_score=harmonic)
print(f"Geometric averaging ROC-AUC is: {ras:0.5f}")

n = 3
mean_of_powers = np.mean(proba**n, axis=1)**(1/n)
ras = roc_auc_score(y_true=y_test, y_score=mean_of_powers)
print(f"Mean of powers averaging ROC-AUC is: {ras:0.5f}")

logarithmic = np.expm1(np.mean(np.log1p(proba), axis=1))
ras = roc_auc_score(y_true=y_test, y_score=logarithmic)
print(f"Logarithmic averaging ROC-AUC is: {ras:0.5f}")
```
- Weighted averages
```python
cormat = np.corrcoef(proba.T)
np.fill_diagonal(cormat, 0.0)
W = 1 / np.mean(cormat, axis=1)
W = W / sum(W) # normalizing to sum==1.0
weighted = proba.dot(W)
ras = roc_auc_score(y_true=y_test, y_score=weighted)
print(f"Weighted averaging ROC-AUC is: {ras:0.5f}")
```
- Correcting averaging for ROC-AUC evaluations
```python
from sklearn.preprocessing import MinMaxScaler

proba = np.stack(
	[model_1.predict_proba(X_train)[:, 1],
	model_2.predict_proba(X_train)[:, 1],
	model_3.predict_proba(X_train)[:, 1]]).T

arithmetic = MinMaxScaler().fit_transform(proba).mean(axis=1)
ras = roc_auc_score(y_true=y_test, y_score=arithmetic)
print(f"Mean averaging ROC-AUC is: {ras:0.5f}")
```
- Blending models using a meta-model
```python
blender = RandomForestClassifier()
blender.fit(proba, y_holdout)
test_proba = np.stack([model_1.predict_proba(X_test)[:, 1],
model_2.predict_proba(X_test)[:, 1],
model_3.predict_proba(X_test)[:, 1]]).T
blending = blender.predict_proba(test_proba)[:, 1]
ras = roc_auc_score(y_true=y_test, y_score=blending)
print(f"ROC-AUC for non-linear blending {model} is: {ras:0.5f}")
```
- Stacking models together
![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2025-03-02%20%E4%B8%8B%E5%8D%887.32.46.png)
```python
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True, random_state=0)
scores = list()
first_lvl_oof = np.zeros((len(X_train), 3))

fist_lvl_preds = np.zeros((len(X_test), 3))
for k, (train_index, val_index) in enumerate(kf.split(X_train)):
	model_1.fit(X_train[train_index, :], y_train[train_index])
	first_lvl_oof[val_index, 0] = model_1.predict_proba(
		X_train[val_index, :])[:, 1]
	model_2.fit(X_train[train_index, :], y_train[train_index])
	first_lvl_oof[val_index, 1] = model_2.predict_proba(
		X_train[val_index, :])[:, 1]
	model_3.fit(X_train[train_index, :], y_train[train_index])
		first_lvl_oof[val_index, 2] = model_3.predict_proba(
	X_train[val_index, :])[:, 1]

model_1.fit(X_train, y_train)
fist_lvl_preds[:, 0] = model_1.predict_proba(X_test)[:, 1]

model_2.fit(X_train, y_train)
fist_lvl_preds[:, 1] = model_2.predict_proba(X_test)[:, 1]

model_3.fit(X_train, y_train)
fist_lvl_preds[:, 2] = model_3.predict_proba(X_test)[:, 1]

second_lvl_oof = np.zeros((len(X_train), 3))
second_lvl_preds = np.zeros((len(X_test), 3))

for k, (train_index, val_index) in enumerate(kf.split(X_train)):
	skip_X_train = np.hstack([X_train, first_lvl_oof])
	model_1.fit(skip_X_train[train_index, :],
		y_train[train_index])
	second_lvl_oof[val_index, 0] = model_1.predict_proba(
		skip_X_train[val_index, :])[:, 1]
	model_2.fit(skip_X_train[train_index, :],
		y_train[train_index])
	second_lvl_oof[val_index, 1] = model_2.predict_proba(
		skip_X_train[val_index, :])[:, 1]
	model_3.fit(skip_X_train[train_index, :],
		y_train[train_index])
	second_lvl_oof[val_index, 2] = model_3.predict_proba(
		skip_X_train[val_index, :])[:, 1]

skip_X_test = np.hstack([X_test, fist_lvl_preds])
model_1.fit(skip_X_train, y_train)
second_lvl_preds[:, 0] = model_1.predict_proba(skip_X_test)[:, 1]
model_2.fit(skip_X_train, y_train)
second_lvl_preds[:, 1] = model_2.predict_proba(skip_X_test)[:, 1]
model_3.fit(skip_X_train, y_train)
second_lvl_preds[:, 2] = model_3.predict_proba(skip_X_test)[:, 1]

arithmetic = second_lvl_preds.mean(axis=1)
ras = roc_auc_score(y_true=y_test, y_score=arithmetic)
scores.append(ras)
print(f"Stacking ROC-AUC is: {ras:0.5f}")
```
- [kaggle-environments/README.md at master · Kaggle/kaggle-environments](https://github.com/Kaggle/kaggle-environments/blob/master/README.md)
- 
---
### LMSYS
- https://www.kaggle.com/competitions/lmsys-chatbot-arena
- NLP classification based on LLM
### **1st Place Solution ➡️ Distill is all you need**
- https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527629
- Data set
  - ut data([https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/499756](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/499756))
  - 33k data ([https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/500973](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/500973))
- **Base models**
  - llama3 70b qwen2 72b gemma2-9b
- Base model architecture: AutoModelForSequenceClassification
```python
lora(9b)
qlora(llama3 and qwen2)
all linear for lora
r=64,a=128
max_len=1024
epoch = 2
global batch_size = 64
```
- Post-pretrain
  - To begin with, train one epoch on three models using the ut dataset. （lr=1e-5）
- Get the logits distribution
  - Load the weight from Post pretrain, split the dataset into 5 folds for training
  - (eg:train➡️4/5 kaggle train data + 33k data,dev➡️1/5 kaggle train data) to train 
    - llama3 70b and qwen2 72b.
  - Then infer the probability distribution of the training set.
- **Distill to the 9b model with logits**
  - After obtaining the logits distribution, load the 9b model for fine-tuning and incorporate the distillation loss during the fine-tuning process. 
  - at least three loss for training，lr=5e-5
- **Model ensemble**
  - Directly average the LoRA layers of the 5 folds.
- **Get 8bit Model**
  - Quantize to 8-bit using GPTQ and use TTA (length 2000) during submission.
- CV/LB
  - infer code: [https://www.kaggle.com/code/sayoulala/v7-gemma-gptq?scriptVersionId=191029701](https://www.kaggle.com/code/sayoulala/v7-gemma-gptq?scriptVersionId=191029701)
  - train code:[https://github.com/shyoulala/LMSYS_BlackPearl](https://github.com/shyoulala/LMSYS_BlackPearl)
- Summary
  - In my solution, the most important aspect is distillation using larger models. 
  - There are also some other details that you can explore on your own if interested. 
  - I believe distillation is a very promising approach, especially in the current Kaggle competitions, where inference constraints are a limiting factor.
  - https://github.com/BlackPearl-Lab/KddCup-2024-OAG-Challenge-1st-Solutions
- Others
  - A100 80G*8, zero2+dp, LLM for sequence classification, batch size=8, max length=1024 and qlora.
- Code understand
  - https://github.com/shyoulala/LMSYS_BlackPearl
---
### **NeurIPS - Ariel Data Challenge 2024**
- https://www.kaggle.com/competitions/ariel-data-challenge-2024
![](Kaggle%20tips/image.png)
- Evaluation:
![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-11-23%2012.03.51.png)
- Data
  - 174.33GB
  - 表征系外行星的化学成分是天文学中一个重要的活跃项目。欧洲航天局的**大气遥感红外系外行星大规模调查（ARIEL）**任务将通过观察系外行星在过境其宿主恒星时的数据，收集约1000颗系外行星的数据。即便ARIEL搭载了强大的仪器，所得数据仍然基于有限数量的光子，并且会包含相当多的噪音。
  - 在本次竞赛中，你的挑战是使用模拟的ARIEL数据提取系外行星大气层的化学光谱。
  - 本次竞赛使用了隐藏测试集。当你提交的笔记本被评分时，实际的测试数据（包括完整的样本提交）将会提供给你的笔记本。你将会看到大约800颗系外行星的数据在隐藏测试集中。
  - 测试集中的一些系外行星模拟数据是直接基于真实的系外行星数据的。这些数据在评分时将被忽略。
#### 1st Solution
- https://www.kaggle.com/competitions/ariel-data-challenge-2024/discussion/544317
- https://www.kaggle.com/competitions/ariel-data-challenge-2024/discussion/544316
- https://www.kaggle.com/code/cnumber/neurips-ariel-data-challenge-2024-final-submission
- Idea
  - 我们解决方案中的许多想法是在深入研究用于数据生成的 ExoSim2 和 TauREx3 代码时发现的，包括增益漂移拟合和前景处理。
  - 虽然我们没有（或无法）发现任何泄漏，但我们必须承认我们的解决方案在某种程度上“破解”了模拟器。
  - 尽管如此，我们希望主办方能够借鉴我们解决方案中的一些方面，并且希望它对实际数据有用。
  - 对于那些好奇我们在比赛最后两天得分显著提高的人，我们决定在另一篇帖子中展示我们团队的具体情况。
- Signal preprocess
  - 我们只使用了 **AIRS-CH0** 通道，因为我们的解决方案主要依赖于相邻波长之间的相关性，而且由于 **FGS1** 通道数据存在失真和波动的点扩散函数（PSF）问题，我们不确定如何有效地利用其数据。
  - 我们使用了一个公共笔记本（我们本想提供链接，但找不到我们用过的笔记本），并做了以下更改：
    1. 禁用热像素处理：
       - 禁用热像素处理后，得分在排行榜上显著提高。
       - 我们认为热像素处理导致了中间区域像素的无效信息丢失，使得几乎无法纠正由时间依赖的 PSF 失真引入的噪声（见下图）。
       - 或许是 **sigma clip** 算法做错了事情，且存在其他算法能够更好地处理热像素，但我们未能找到这些算法。
    2. 前景处理：
       - 一些团队已经注意到，将最终光谱乘以约 1.006~1.008 的系数能显著提高结果。
       - 这是因为在 ExoSim2 仿真过程中，信号中加入了前景信号（[链接到 GitHub]）。
       - 处理这种情况的正确方法是估算与波长相关的前景信号，并将其从中心区域信号中减去。
       - 在我们的解决方案中，我们选择用区域 **[0:8]** 和 **[24:32]** 来估算前景信号，并从中心区域 **[8:24]** 中减去它。
       - 考虑前景的正确效果可以在此讨论帖中看到。[NeurIPS - Ariel Data Challenge 2024](https://www.kaggle.com/competitions/ariel-data-challenge-2024/discussion/543853#3034987)
- Initial Dip Estimation for Each Wavelength
  1. 识别过境区间
     - 为了识别过境的时间位置，我们使用了波长平均信号。
     - 首先通过基于规则的算法估计出大致的时间位置，然后使用拟合算法精确确定具体时间。
  2. 增益漂移（Gain Drift）
     - 增益漂移指的是探测器的增益随着时间和不同波长的变化。
     - 这些漂移可能会引入系统误差，影响观测信号，因此必须对其进行修正，才能准确估计过境的下降（transit dip）。
     - 增益漂移的修正是一个关键步骤，确保我们能准确捕捉信号中由于过境事件引起的光谱变化。
- Modeling
![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-11-23%2012.24.55.png)
- **下降误差估计与自助法（Bootstrapping）**
  - 在每个波长的下降估计中，由于每个波长的信噪比不同，导致下降估计的误差也不同。为
  - 了解决这个问题，并估计每个波长的下降误差，使用了 **自助法（Bootstrapping）**。
  - 通过自助法，我们可以从样本数据中重复抽取样本，并对每次抽样计算误差，从而估计误差的分布。
  - 具体来说，自助法有助于克服不同波长之间信号噪声比差异所带来的问题，使得每个波长的误差估计更加准确。
  - 关于自助法的更多细节，可以在我们的代码中找到。
- 考虑波长相关性的下降估计
  - 在我们的解决方案中，使用了三种模型来进行波长相关性的下降估计：
    - **高斯过程回归（Gaussian Process Regression）**
    - **自编码器（AutoEncoder）** 
    - **非负矩阵分解（NMF）**。
    - 这三种模型被结合成一个集成模型，比例为 **6:2:2**。
- **高斯过程回归（Gaussian Process Regression）**
  - 这部分的实现相对简单，并不像第二名的解决方案那样复杂。
  - 我们使用了一个简单的核函数，它由 **RBF（径向基函数）** 和 **Matern 核函数** 组成。
    - 通过这种方法，我们能够计算每个数据点的误差，并将这些误差传递给 sklearn.gaussian_process.GaussianProcessRegressor 模型，以便模型能够考虑每个数据点的不确定性。
    * **RBF 核函数** 用于建模平滑的函数关系。
    * **Matern 核函数** 用于处理一些非平滑的、具有较长相关长度的情况。
  - 通过高斯过程回归模型，我们能够更好地建模波长间的相关性，并且考虑到每个数据点的误差不确定性，进一步提高了下降估计的精度。
- **自动编码器（AutoEncoder）**
  - 我们应用了自动编码器（AutoEncoder）来捕捉数据中的关系。
    - 与**主成分分析（PCA）**不同，后者只能捕捉线性关系，自动编码器能够建模更复杂且非线性的模式。
    - 因此，自动编码器非常适合处理涉及非线性关系的信号，如系外行星的过境信号。
  - 此外，我们预期通过使用均方误差（MSE）损失函数来训练模型，自动编码器能够考虑噪声并恢复出“最优”光谱。
    - 这使得模型能够在处理带有噪声的观测数据时，仍然有效地捕捉信号的真实模式。
  - **关键点：**
    1. **数据归一化**：对于每个系外行星，我们对数据进行了归一化处理。
       - 这有助于在不同的观测条件下，确保模型训练时不会受到不同规模的影响。
    2. **移动中位数平滑**：为了平滑输入的过境光谱，我们对每个过境信号应用了**移动中位数**方法。
       - 这一处理步骤能够去除短期波动，并保持数据的主要趋势。
  - **模型结构：**我们使用的自动编码器结构如下：
    * **输入层**：与输入数据维度一致。
    * **隐藏层**：包含4个节点。
    * **输出层**：恢复输入信号的重构。
  - 通过训练自动编码器，我们能够从带噪声的输入光谱中恢复出“最佳”光谱，即去除噪声并捕捉到系外行星过境信号中的真实变化。
```python
input_data = Input(shape=(input_dim,))
encoded = Dense(encoding_dim, activation='relu')(input_data)
decoded = Dense(input_dim, activation='linear')(encoded)

autoencoder = Model(input_data, decoded)
```
- 非负矩阵分解（NMF）
  - NMF（Non-negative Matrix Factorization）在我们的解决方案中与自动编码器（AutoEncoder）非常相似，但我们添加了NMF以增强模型的多样性。
    - NMF作为一种矩阵分解技术，特别适用于分解具有非负约束的数据，如光谱数据，这与自动编码器类似，但它在数学上有所不同。
  - NMF与自动编码器的区别：
    * **NMF**：我们为NMF设置了**5个秩**（rank），这意味着我们希望分解光谱数据为5个潜在的谱成分。每个谱成分代表数据中某种隐含的模式或特征。
    * **自动编码器**：则侧重于通过神经网络学习非线性特征，通常通过调整隐藏层节点来优化模型。
  - NMF识别的光谱成分
    - 使用NMF对训练数据进行分解后，模型识别出了以下主要光谱成分：
      * **成分 1**：CO₂（二氧化碳）
      * **成分 2**：CH₄（甲烷）
      * **成分 3**：H₂O（水蒸气）
    - 这些成分代表了大气层中主要的贡献气体，它们在不同的成分中有不同的相对贡献。
  - NMF的优势
    - NMF的一个显著优势是它能够无监督地考虑光谱之间的相关性。
    - 通过这种方法，NMF可以从光谱数据中自动识别出不同的气体成分，无需先验的标签或监督信息，这使其在处理未知数据时非常有用。
![](Kaggle%20tips/image%202.png)
- **Sigma（标准差）估计**
  - 在我们的解决方案中，我们通过加权平均以下几个成分来估算 **Sigma**（即信号的不确定性或标准差）：
    1. **常数值**（与行星和波长无关）：
       - 这是一个固定的值，用于捕捉信号中始终存在的基础噪声。
    2. **平滑的预测过境光谱的标准差**（与行星相关，但与波长无关）：
       - 这是我们对过境光谱的平滑版本计算出的标准差，反映了每个时刻行星的信号噪声。
    3. **通过高斯过程回归预测的不确定性**（与行星和波长相关）：
       - 高斯过程回归模型提供的预测不确定性，这个不确定性既取决于行星，也取决于波长。
  - 常数值的重要性
    - 常数值在一些情况下起到了重要作用，特别是当过境光谱几乎保持不变，但存在一定的偏差时。
    - 这个常数值帮助我们捕捉到这种偏差，并使得模型能够更好地处理接近恒定的信号。
![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-11-23%2013.02.38.png)
- Not work
  - 已知气体的吸收光谱
    - 我们尝试使用 **TauREx3** 来拟合过境光谱。
    - 尽管它在训练数据上表现得非常好，但排行榜上的得分却非常糟糕，可能是由于大气成分的变化造成的偏移。
  - 我们确实尝试在测试数据中识别气体，但未能成功。
  - **使用机器学习去噪输入数据**
    - 假设取 [8:24] 通道的总和是最优的，这一点实际上是很难成立的。
    - 在某些情况下，最优范围可能是 [9:23] 或 [10:22]，甚至使用加权和可能是最优的。
    - 为了解决这个问题，我们尝试了几种机器学习方法，但没有能够超越 [8:24] 的结果，可能是由于信号的时空波动性。
- Final
  - 对仿真代码的深入分析为我们的胜利提供了关键的思路。
  - 如果没有观察 **ExoSim2** 代码，我们可能无法发现增益漂移函数或前景处理方法。
  - 虽然我们原本认为使用仿真生成数据的竞赛注定会依赖于这些技术，但我们非常惊讶地发现，一些顶尖团队能够在没有使用这些方法的情况下取得高分，因此再次为他们鼓掌。
- During the last 2 days
  - 转折点发生在我们发现 **去除热像素处理**后，尽管对训练得分没有太大影响，但 **排行榜得分**（LB Score）显著提高了 **+0.015**。
  - 更重要的是，去除热像素处理后，之前无法在排行榜上有效提升得分的多个过程——如 **两阶段拟合**、**自动编码器（AutoEncoder）** 和 **非负矩阵分解（NMF）**——突然间起作用了。
  - 此外，我们在最终提交时，幸运地找到了 **Sigma 的正确调整方法** 等优化措施。
  - 如果没有这些改进，我们的第二次提交是无法夺得第一名的。
![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2024-11-23%2013.09.28.png)<!-- {"width":818} -->
![](Kaggle%20tips/image%203.png)
---
## **TReNDS Neuroimaging**
- https://www.kaggle.com/c/trends-assessment-prediction/data
### Data and task
- 在这项竞赛中，你将根据多模态脑部 MRI 特征预测多个评估结果以及年龄。你将基于其他数据科学家的现有结果进行工作，完成验证多模态特征在正常人群（无病影响的受试者）中的有效性的关键任务。由于大脑的复杂性和扫描仪之间的差异，通用方法对于有效推动多模态神经影像学研究向前发展至关重要。
- 在本次挑战中，参与者将从脑部 MRI 图像提取特征，并预测来自两个领域的年龄和评估值。
  - 模型需要能够在来自不同扫描仪/站点（站点 2）上的数据上进行泛化。
  - 站点 2 的所有受试者已被分配到测试集中，因此他们的分数不可用。
  - 尽管测试集中的站点 2 受试者数量少于站点 1 受试者，但站点 2 的总受试者数量将在竞赛结束后才会公布。
  - 为了增加挑战性，部分站点 2 受试者的 ID 已经在下文揭示。
  - 请使用这些信息来帮助你的模型识别站点效应。站点效应是一种偏倚。
  - 为了良好地泛化，模型应该学习与站点效应无关或不受站点效应驱动的特征。
  - 本次竞赛的 **.mat** 文件可以使用 **h5py** 在 Python 中读取，**.nii** 文件可以使用 **nilearn** 在 Python 中读取。
- 1.**fMRI_train** 
  - 一个文件夹，包含 53 个训练样本的 3D 空间图（.mat 格式）。
- 2.**fMRI_test** 
  - 一个文件夹，包含 53 个测试样本的 3D 空间图（.mat 格式）。
- 3.**fnc.csv** 
  - 静态 FNC 相关特征，适用于训练和测试样本。
- 4.**loading.csv** 
  - 静态 MRI SBM 负荷数据，适用于训练和测试样本。
- 5.**train_scores.csv** 
  - 训练样本的年龄和评估值。
- 6.**sample_submission.csv** - 一个示例提交文件，格式正确。
- 7.**reveal_ID_site2.csv** 
  - 一个包含站点 2 受试者 ID 的列表，这些数据是使用与训练样本不同的扫描仪收集的。
- 8.**fMRI_mask.nii** - 一个 3D 二进制空间图。
- 9.**ICN_numbers.txt** - 每个 fMRI 空间图的内在连接网络编号；与 FNC 名称相匹配。
### Solutions
#### **1st private (1st public) place + code**
- https://www.kaggle.com/competitions/trends-assessment-prediction/discussion/163017
- https://github.com/DESimakov/TReNDS/
- The core of our approach is:
  - I. Generate powerful features from 3D fMRI data.
  - II. Train-test “matching”.
  - III. Training of a diverse ensemble (different models and feature subsets).
  - IV. Сombine all together.
- **I. Generate features from 3D fMRI data.**
  - We tried to use 3D CNN models at first, but did not spend so much time on it.
  - Small network (resnet10 without pre-training) with AdamW, L1 regularization, [ard](https://github.com/HolyBayes/pytorch_ard) layers, snapshots and CutOut3D augmentation worked better as a regressor. 
    - 3D auto encoders (simple or UNet-like) + PCA (or ICA) also worked, but was slightly worse for us.
  - The score overall was quite low (only age and domain2-var2 were higher than median prediction) and composing with others features was difficult
    - (3D CNN model trains 10 epochs, whereas MLP on loadings and fnc 70 epochs). 
    - So we (mostly [@simakov](https://www.kaggle.com/simakov)) decided that it was too hard problem for many of top20 participants and they must use something different (it is great that we are wrong, your solutions are cool and interesting!).
![](Kaggle%20tips/image%204.png)
#### 2nd solution
- [TReNDS Neuroimaging](https://www.kaggle.com/competitions/trends-assessment-prediction/discussion/162765)
![](Kaggle%20tips/image%205.png)
![](Kaggle%20tips/image%206.png)
- Summary
  - SVM and linear model works well in tabular data, but tree-based model doesn’t work as well as KNN
  * 3D ResNet18 is best CNN here for me. Comparing to that, ResNet50, ResNext50, ResNext101 doesn’t work well.
  * Splitting 3D fMRI into 3~6 pieces gave me boost, but 8 pieces are useless.
#### 3rd Solution
- https://www.kaggle.com/competitions/trends-assessment-prediction/discussion/162934
- [shimacos37/kaggle-trends-3rd-place-solution: the 3rd place solution code of Kaggle TReNDS Neuroimaging \(https://www.kaggle.com/c/trends-assessment-prediction/overview\)](https://github.com/shimacos37/kaggle-trends-3rd-place-solution)
![](Kaggle%20tips/image%207.png)
- Solution Details
  - I think that the point of this competition is stacking and diversity of models.
  - So, I built many models as shown below.
- P1: Simple models by table feature.
  - First, I trained simple models by fnc and loding feature.
  * SVM (rbf kernel and linear kernel)
  * NuSVM
  * KNN
  * Ridge
  * BaggingRegressor(Ridge)
* P2: NN models by fMRI data.
  * 2-1. 3D voxel
    * I applied sample wise and component wise normalization.
      * ResNet18 replaced by 3D modules.
    * Simple 3dConvNet (smaller parameter than ResNet18)
      * trained by all label
      * trained by age, domain1_var1 and domain1_var2
    * (2 + 1) D CNN (suggested in [A Closer Look at Spatiotemporal Convolutions for Action Recognition](https://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_A_Closer_Look_CVPR_2018_paper.pdf))
      * better than ResNet18
  * 2-2. masked data
    * [Masker of Nilearn](https://nilearn.github.io/modules/generated/nilearn.input_data.NiftiLabelsMasker.html#nilearn.input_data.NiftiLabelsMasker) can extract signals from brain parcellation. 
      * Then, I used [schaefer_2018](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_schaefer_2018.html#nilearn.datasets.fetch_atlas_schaefer_2018) parcellation and extracted 400 rois from fMRI. 
      * (reduce shape [53, 52, 64, 53] to [53, 400])
    - Finally, I built some models by this [53, 400] signals.
    * CNN models (input channel is 400)
      * 1D ResNet18
      * 1D densenet121
      * 1D ResNest 14d
    * Transformer
      * No PositionalEncoding
      * Self Attention of components by components (feature axis is 400)
    * GIN (Graph Isomorphism Network)
      * Impremented by [Deep Graph Library](https://www.dgl.ai/)
      * I used the network architecture like in [this paper](https://arxiv.org/pdf/2001.03690.pdf).
* P3: **LGBM and XGBoost with table features, NN features and voxel statistical features.**
  * I computed voxel statistical feature (mean, max, kurt, skew) of each components.
    * I used NiftiMasker for extracting non-zero area.
  * LGBM (XGBoost) model with hidden feature of (2 + 1)D CNN
  * LGBM (XGBoost) model with hidden feature of Simple 3dConvNet
  * LGBM (XGBoost) model with hidden feature of GIN
  * I selected 1024 feature of each models by feature importance and reduced the number of leaves to avoid over-fitting (n_leaves = 2).
* P4: Stacking
  * I just looked at the distribution of predictions in the training and test data and erased them if there were too much discrepancy. 
  * And I made interaction feature between domains (sum, abs(diff), multiply). 
  * After the preprocessing, I used Linear SVM and LGBM for stacking and finally blending the predictions.
* **Some Insights**
  * 3DCNN with table feature had better CV but this model was useless when stacking.
  * Removing noise prediction had important role on shake-up.
  * MONAI framework is too slow. [Rising](https://github.com/PhoenixDL/rising) is faster and I used it. But I don't know if it helped to boost the score…
    * Rising is a high-performance data loading and augmentation library for 2D *and* 3D data completely written in PyTorch. 
* **What didn't work**
  * Pseudo label
  * Ridge stacking, XGBoost stacking, Second level stacking
  * GNN with fnc attention
  * Select less feature of lgbm and xgboost than 1024
  * https://tomatosauce.jp/adversarial_validation/
---
## **Santander Product Recommendation**
- 20161214
- https://www.kaggle.com/competitions/santander-product-recommendation/discussion/26899
### Data and information 
- 在本次竞赛中，您将使用来自桑坦德银行（Santander）的1.5年的客户行为数据，预测客户将购买哪些新产品。
  - 数据从2015年1月28日开始，包含每个月客户所拥有的产品记录，例如“信用卡”、“储蓄账户”等。
  - 您将预测客户在2016年6月28日所会获得的附加产品，这些产品将是在2016年5月28日他们已经拥有的基础上增加的。
  - 这些产品是名为：ind_(xyz)_ult1的列，它们在训练数据中的位置是第25列到第48列。
  - 您需要预测客户除了在2016年5月28日已经拥有的产品外，还会购买哪些新产品。
- Metric: MAP@7
![](Kaggle%20tips/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202024-12-02%2012.43.06.png)
### **3rd place solution**
- Summary
![](Kaggle%20tips/overview.png)
- Models
  - 每个产品的新购买概率（记作Pr）是通过xgboost模型计算得出的（针对20个产品，除了ahor_fin、aval_fin、deco_fin和deme_fin）。
    - **cco_fin**的Pr只通过2015年12月28日的数据进行预测。
    - **reca_fin**的Pr只通过2015年6月28日的数据进行预测。
  * 其余18个产品的Pr分别通过2016年5月28日、2016年4月28日、2016年3月28日、2016年2月28日、2016年1月28日和2015年12月28日的数据进行预测。
  * 其余18个产品的Pr还通过2015年12月28日至2016年4月28日的数据进行预测
    * 且仅使用有新购买产品的记录。
  * **ahor_fin**、**aval_fin**、**deco_fin**和**deme_fin**的Pr被固定为1^-10。
- Features
  - 使用的特征数量为142个，具体如下所示：
    - **原始特征**，除了以下特征外：
      - ‘fecha_dato’、‘ncodpers’、‘fecha_alta’、‘ult_fec_cli_1t’、‘tipodom’和’cod_prov’（18个特征）
    - **‘ind_actividad_cliente’ 和该特征上个月的值的连接**（1个特征）
    - **‘tiprel_1mes’ 和该特征上个月的值的连接**（1个特征）
    - **20个产品的上个月的值**（20个特征）
    - **上述20个特征的拼接，作为字符**（1个特征） —> Caution: num cat as a string
    - **上个月购买的产品数量**（1个特征）
    - **20个产品的索引变化模式的计数**（包括0到0，0到1，1到0，1到1的变化），直到上个月（80个特征）--> Caution: Change pattern count
    - **20个产品的连续0索引的长度，直到上个月**（20个特征）
  - Character (factor) variables are replaced with target mean in each modeling process.
    - Target encoder
- Others
  - I basically used '2016-05-28' data as a validation dataset, but when I used '2016-05-28' data as a train dataset, I used '2016-04-28' data as a validation dataset.
- QA
  - Why have we chosen different dates data for training for different products?
    - Because some products have strong seasonal trend.
  - Why **ahor_fin**、**aval_fin**、**deco_fin**和**deme_fin**的Pr被固定为1^-10?
    - These product are purchased very rarely, and there are not enough positive training data.
    - I thought that they have almost 0 probability to be purchased, so I fixed their probability to 1e-10 without constructing models of them.
  - Target encode: mean of the target (response) variables
    - he ratio of people who purchased the product newly
---
## **Facebook V: Predicting Check Ins**
### Data and information
- 在本次竞赛中，您将根据用户的位置、精度和时间戳预测用户正在签到的商家。
- 训练集和测试集是根据时间进行划分的，测试数据中的公共/私人排行榜是随机划分的。该数据集没有“人”的概念，所有的 row_id 都代表事件，而不是个人。
- 注意：某些列，例如时间和准确度，故意在定义上保持模糊。请将这些作为挑战的一部分来考虑。
- train.csv, test.csv
  - row_id：签到事件的 ID
  - x y：坐标
  - accuracy：位置精度
  - time：时间戳
  - place_id：商家的 ID，这是您要预测的目标
### **1st Place - Winning Solution**
- https://www.kaggle.com/competitions/facebook-v-predicting-check-ins/discussion/22081
- https://github.com/ttvand/Facebook-V
- 




---
## **Child Mind Institute - Detect Sleep States**
### Data and information
- 

### 14th Place Solution
- https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states/discussion/460274



---
## **Backpack Prediction Challenge**
### Data and information
- Tabular 
- Price prediction
- RMSE
- Data set like
![](Kaggle%20tips/%E6%88%AA%E5%B1%8F2025-03-01%20%E4%B8%8B%E5%8D%883.28.28.png)
### 1st Place Solution
- [First Place - Single Model - \[LB 38.81\]](https://www.kaggle.com/code/cdeotte/first-place-single-model-lb-38-81)
#### Feature Engineering with Fast cuDF-Pandas!
- One of the most powerful feature engineering techniques is groupby(COL1)[COL2].agg(STAT)
- Cuda
  - We can write cuDF code which looks just like Pandas code and starts with `import cudf`. 
  - Or we can write normal Pandas code with import pandas but before that we add the cell magic command `%load_ext cudf.pandas`. 
- **RAPIDS cuDF 25.02 cuML 25.02**
  - https://www.kaggle.com/code/cdeotte/rapids-cudf-25-02-cuml-25-02
```python
%load_ext cudf.pandas

import numpy as np, pandas as pd
import matplotlib.pyplot as plt
pd.set_option('display.max_columns', 500)

VER=1
```
#### Feature Engineer Columns
- We will engineer 8 new columns by combining existing columns.
```python
CATS = list(train.columns[1:-2])
print(f"There are {len(CATS)} categorical columns:")
print( CATS )
print(f"There are 1 numerical column:")
print( ["Weight Capacity (kg)"] )

COMBO = []
for i,c in enumerate(CATS):
    #print(f"{c}, ",end="")
    combine = pd.concat([train[c],test[c]],axis=0)
    combine,_ = pd.factorize(combine)
    train[c] = combine[:len(train)]
    test[c] = combine[len(train):]
    n = f"{c}_wc"
    train[n] = train[c]*100 + train["Weight Capacity (kg)"]
    test[n] = test[c]*100 + test["Weight Capacity (kg)"]
    COMBO.append(n)
print()
print(f"We engineer {len(COMBO)} new columns!")
print( COMBO )

FEATURES = CATS + ["Weight Capacity (kg)"] + COMBO
print(f"We now have {len(FEATURES)} columns:")
print( FEATURES )
```
- Xgboost with stats features
```python
# STATISTICS TO AGGEGATE FOR OUR FEATURE GROUPS
STATS = ["mean","std","count","nunique","median","min","max","skew"]
STATS2 = ["mean","std"]

FOLDS = 7
kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)

oof = np.zeros((len(train)))
pred = np.zeros((len(test)))

# OUTER K FOLD
for i, (train_index, test_index) in enumerate(kf.split(train)):
    print(f"### OUTER Fold {i+1} ###")

    X_train = train.loc[train_index,FEATURES+['Price']].reset_index(drop=True).copy()
    y_train = train.loc[train_index,'Price']

    X_valid = train.loc[test_index,FEATURES].reset_index(drop=True).copy()
    y_valid = train.loc[test_index,'Price']

    X_test = test[FEATURES].reset_index(drop=True).copy()

    # INNER K FOLD (TO PREVENT LEAKAGE WHEN USING PRICE)
    kf2 = KFold(n_splits=FOLDS, shuffle=True, random_state=42)   
    for j, (train_index2, test_index2) in enumerate(kf2.split(X_train)):
        print(f" ## INNER Fold {j+1} (outer fold {i+1}) ##")

        X_train2 = X_train.loc[train_index2,FEATURES+['Price']].copy()
        X_valid2 = X_train.loc[test_index2,FEATURES].copy()

        ### FEATURE SET 1 (uses price) ###
        col = "Weight Capacity (kg)"
        tmp = X_train2.groupby(col).Price.agg(STATS)
        tmp.columns = [f"TE1_wc_{s}" for s in STATS]
        X_valid2 = X_valid2.merge(tmp, on=col, how="left")
        for c in tmp.columns:
            X_train.loc[test_index2,c] = X_valid2[c].values

        ### FEATURE SET 2 (uses price) ###
        for col in COMBO:
            tmp = X_train2.groupby(col).Price.agg(STATS2)
            tmp.columns = [f"TE2_{col}_{s}" for s in STATS2]
            X_valid2 = X_valid2.merge(tmp, on=col, how="left")
            for c in tmp.columns:
                X_train.loc[test_index2,c] = X_valid2[c].values

    ### FEATURE SET 1 (uses price) ###
    col = "Weight Capacity (kg)"
    tmp = X_train.groupby(col).Price.agg(STATS)
    tmp.columns = [f"TE1_wc_{s}" for s in STATS]
    X_valid = X_valid.merge(tmp, on=col, how="left")
    X_test = X_test.merge(tmp, on=col, how="left")

    ### FEATURE SET 2 (uses price) ###
    for col in COMBO:
        tmp = X_train.groupby(col).Price.agg(STATS2)
        tmp.columns = [f"TE2_{col}_{s}" for s in STATS2]
        X_valid = X_valid.merge(tmp, on=col, how="left")
        X_test = X_test.merge(tmp, on=col, how="left")

    ### FEATURE SET 3 (does not use price) ###
    for col in CATS:
        col2 = "Weight Capacity (kg)"
        tmp = X_train.groupby(col)[col2].agg(STATS2)
        tmp.columns = [f"FE3_{col}_wc_{s}" for s in STATS2]
        X_train = X_train.merge(tmp, on=col, how="left")
        X_valid = X_valid.merge(tmp, on=col, how="left")
        X_test = X_test.merge(tmp, on=col, how="left")

    # CONVERT TO CATS SO XGBOOST RECOGNIZES THEM
    X_train[CATS] = X_train[CATS].astype("category")
    X_valid[CATS] = X_valid[CATS].astype("category")
    X_test[CATS] = X_test[CATS].astype("category")

    # DROP PRICE THAT WAS USED FOR TARGET ENCODING
    X_train = X_train.drop(['Price'],axis=1)

    # BUILD MODEL
    model = XGBRegressor(
        device="cuda",
        max_depth=6,  
        colsample_bytree=0.5, 
        subsample=0.8,  
        n_estimators=10_000,  
        learning_rate=0.02,  
        enable_categorical=True,
        min_child_weight=10,
        early_stopping_rounds=100,
    )
    
    # TRAIN MODEL
    COLS = X_train.columns
    model.fit(
        X_train[COLS], y_train,
        eval_set=[(X_valid[COLS], y_valid)],  
        verbose=300,
    )

    # PREDICT OOF AND TEST
    oof[test_index] = model.predict(X_valid[COLS])
    pred += model.predict(X_test[COLS])

pred /= FOLDS
```
- Feature importance
```python
import xgboost as xgb
fig, ax = plt.subplots(figsize=(10, 20))
xgb.plot_importance(model, max_num_features=100, importance_type='gain',ax=ax)
plt.title("Top 100 Feature Importances (XGBoost)")
plt.show()
```

